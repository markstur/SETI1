{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.cognitiveclass.ai\"><img src = \"https://cognitiveclass.ai/wp-content/themes/bdu3.0/static/images/cc-logo.png\" align = left></a>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "--------------------\n",
    "# Search for Extra Terrestrial Intelligence (SETI)\n",
    "###  SETI Signal Classification on PowerAI with Multi GPU\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Introduction\n",
    "In this Notebook, we will use the famous [SETI Dataset](https://github.com/setiQuest/ML4SETI/) to build a Convolutional Neural Networks capable to perform signals classification. CNN will say, with some associated error, what type of signal is the presented input. In this notebook, you will use IBM PowerAI with multiple GPU to train the model.\n",
    "\n",
    "### Project overview:\n",
    "Each night, using the Allen Telescope Array (ATA) in northern California, the SETI Institute scans the sky at various radio frequencies, observing star systems with known exoplanets, searching for faint but persistent signals. The current signal detection system is programmed to search only for particular kinds of signals: narrow-band carrier waves. However, the detection system sometimes triggers on signals that are not narrow-band signals  (with unknown efficiency) and are also not explicitly-known radio frequency interference (RFI). There seems to be various categories of these kinds of events that have been observed in the past.\n",
    "\n",
    "Our goal is to classify these accurately in real-time. This may allow the signal detection system to make better observational decisions, increase the efficiency of the nightly scans, and allow for explicit detection of these other signal types.\n",
    "\n",
    "For more information refer to [SETI hackathon page](https://github.com/setiQuest/ML4SETI/).\n",
    "\n",
    "\n",
    "Framing the radio signal data into spectrogram (a 2D visual representation), we can convert the problem into something akin to an image classification problem. CNN, will be run on the images which are the result of converting the signals to spectrogram.\n",
    "\n",
    "### Training on Multi-GPU:\n",
    "Today, many systems contains multiple GPUs for high performance computation. We can leverage this environments to run the training operation concurrently across multiple cards. One sample of these kind of environments is [IBM PowerAI](http://cocl.us/SETI-NIMBIX-PowerAI). In this notebook, we show you how to design and run your model on multiple GPUs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "#!sudo pip install sklearn\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from six.moves import urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Set the destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/SETI1_data/\n",
      "tmp/SETI1_train/\n"
     ]
    }
   ],
   "source": [
    "### SET YOUR WORKING SPACE HERE! Use this folder to save intermediate results.\n",
    "dataset_name = 'SETI_ds_64x128'\n",
    "data_dir = \"tmp/SETI1_data/\"\n",
    "train_dir = 'tmp/SETI1_train/'\n",
    "log_dir = train_dir + 'log6'\n",
    "#check point directory\n",
    "chk_directory = train_dir + '/save/'\n",
    "checkpoint_path = chk_directory + 'model.ckpt'\n",
    "\n",
    "if os.path.exists(data_dir) is False:\n",
    "    os.makedirs(data_dir)\n",
    "print data_dir\n",
    "\n",
    "\n",
    "if os.path.exists(train_dir) is False:\n",
    "    os.makedirs(train_dir)\n",
    "print train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Import dataset reader\n",
    "The signals for this notebook, have been converted to spectogram images, and stored as 4 files.\n",
    "The following cell will load a python code that help us to decode the binary file, and read the SETI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  SETI.zip\n",
      "  inflating: SETI.py                 \n",
      "  inflating: __MACOSX/._SETI.py      \n"
     ]
    }
   ],
   "source": [
    "!wget -q --output-document  SETI.zip  https://ibm.box.com/shared/static/jhqdhcblhua5dx2t7ixwm88okitjrl6l.zip\n",
    "!unzip -o SETI.zip\n",
    "import SETI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Download data\n",
    "The dataset exist and shared on IBM box. Running the following cell, you can download the dataset and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Successfully downloaded', 'qz33lcio9ip2j8qi2atxqs62gn3bnu2s.gz', 2432541, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_and_extract():\n",
    "    DATA_URL =  'https://ibm.box.com/shared/static/qz33lcio9ip2j8qi2atxqs62gn3bnu2s.gz'\n",
    "    dest_directory = data_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print 'No zip file exist in ', imagefilepath\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    extracted_dir_path = os.path.join(dest_directory, dataset_name)\n",
    "    if not os.path.exists(extracted_dir_path):\n",
    "        print 'Extracting to', extracted_dir_path\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Load data SETI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/SETI1_data/SETI_ds_64x128/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/SETI1_data/SETI_ds_64x128/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/SETI1_data/SETI_ds_64x128/test-images-idx3-ubyte.gz\n",
      "Extracting tmp/SETI1_data/SETI_ds_64x128/test-labels-idx1-ubyte.gz\n",
      "Number of time series: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 8192)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_directory = data_dir + dataset_name\n",
    "dataset = SETI.read_data_sets(ds_directory, one_hot=True, validation_size=0)\n",
    "num_examples = dataset.train.num_examples\n",
    "print 'Number of time series:', num_examples\n",
    "dataset.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Understanding the imported data\n",
    "\n",
    "The imported data can be divided as follow:\n",
    "\n",
    "- Training (dataset.train) >>  Use the given dataset with inputs and related outputs for training of NN. In our case, if you give an image that you know that represents a \"class1\", this set will tell the neural network that we expect a \"class1\" as the output.  \n",
    "        - 2,000 signals (images)\n",
    "        - dataset.train.images for inputs\n",
    "        - dataset.train.labels for outputs\n",
    "  \n",
    "  \n",
    "- Test (dataset.test) >> the model does not have access to this informations prior to the test phase. It is used to evaluate the performance and accuracy of the model against \"real life situations\". No further optimization beyond this point.  \n",
    "        - 8,000 data points\n",
    "        - dataset.test.images for inputs\n",
    "        - dataset.test.labels for outputs\n",
    "        \n",
    "        \n",
    "#### Labels\n",
    "- Each image (spectrum of signal) in the dataset has been labeled from 1 to 4, representing:\n",
    "    - squiggle\n",
    "    - narrowband\n",
    "    - noise\n",
    "    - narrowbanddrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Network Parameters\n",
    "\n",
    "We place the parameters on CPU.   \n",
    "__Notice:__ This code is not optimal for single-GPU training due to the cost of copying parameters between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "decay_rate=0.96  #decay every 1000 steps with a base of 0.96:\n",
    "decay_steps=1500\n",
    "learning_rate = 0.005\n",
    "training_epochs = 5000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "moving_avg_decay = 0.9999     # The decay to use for the moving average.\n",
    "\n",
    "#check point directory\n",
    "chk_directory = train_dir +'/save/'\n",
    "checkpoint_path = chk_directory + 'model.ckpt'\n",
    "\n",
    "\n",
    "num_classes = 4 # number of possible classifications for the problem\n",
    "dropout = 0.50 # Dropout, probability to keep units\n",
    "\n",
    "height = 64 # height of the image in pixels \n",
    "width = 128 # width of the image in pixels \n",
    "n_input = width * height # number of pixels in one image \n",
    "num_GPUs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inputs():\n",
    "    train_img_ds = dataset.train.images\n",
    "    image = tf.convert_to_tensor(train_img_ds, np.float32)\n",
    "    image = tf.reshape(image, [-1,height,width,1], name = 'images')\n",
    "    train_lb_ds = dataset.train.labels\n",
    "    label = tf.convert_to_tensor(train_lb_ds, np.float32)\n",
    "    #label = tf.arg_max(label_onehot,1)\n",
    "    label  = tf.reshape(label, [-1,4], name = 'labels')\n",
    "    # Ensure that the random shuffling has good mixing properties.\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(100 *  min_fraction_of_examples_in_queue)\n",
    "    print ('Filling queue with %d SETI images before starting to train. '\n",
    "    'This will take a few minutes.' % min_queue_examples)\n",
    "    num_preprocess_threads = 16\n",
    "    images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples, enqueue_many=True)\n",
    "    #labels = tf.reshape(label_batch, [batch_size])\n",
    "    return(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### How to use multi GPU for training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "PowerAI supports multi-GPU calculation.  Given multiple GPU cards, we run the SETI model on each GPU as following:\n",
    "\n",
    "    1. Store all model parameters on the CPU, e.g weights and biases.\n",
    "    2. Put a copy of the SETI model on each GPU, including conv2d, pre-activation, relu, etc.\n",
    "    3. Divide up a large batch of data across the GPUs, and feed each model replica with a unique batch of data.\n",
    "    4. Compute the inference on each GPU\n",
    "    5. Calculate the gradients on each GPU\n",
    "    6. Wait until all GPUs finish processing of their batches\n",
    "    7. Update model parameters synchronously on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Variable definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We define all variables using __tf.get_variable()__ in order to place the variables on CPU to be shares across multiple GPU for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    var = _variable_on_cpu( name, shape ,tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "This function builds the graph as far as required for running the network forward to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope: # a sample variable would be conv1/weights\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 1, 32], stddev=0.1,  wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [32], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay('weights', shape=[5, 5, 32, 64],  stddev=0.1,  wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,  name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 4, 4, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "    \n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 1024], stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [1024], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[1024, 256],  stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [256], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        #_activation_summary(local4)\n",
    "\n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', [256, num_classes],  stddev=1/256.0, wd=0.0)\n",
    "        biases = _variable_on_cpu('biases', [num_classes], tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Loss function\n",
    "loss function, adds the ops required to generate loss, to the inference graph. It calculates the average cross entropy loss across the batch.  \n",
    "\n",
    "The total loss in calc_loss is defined as the cross entropy loss plus all of the weight decay terms (L2 loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# # Compute the total loss of the prediction with respect to the labels.\n",
    "def calc_loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As we want to run the CNN on multiple GPUs, we construct our model in a multi-tower fashion where each tower is assigned to a different GPU. The following function calculate the loss value for each tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total loss on a single tower running the SETI model.\n",
    "def tower_loss(scope, logits, labels):\n",
    "    #x_batch, labels = dataset.train.next_batch(batch_size,shuffle=True)\n",
    "    #images = tf.reshape(x_batch, [-1,height,width,1], name = 'image_batch')\n",
    "    \n",
    "    # Build inference Graph.\n",
    "    #logits = inference(images) # it builds a subgraph on twoer with all the operations, to use the shared variables.\n",
    "    #_=calc_loss(logits, labels) # calculate cross_entropy_mean and add it to collection\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    # Assemble all of the losses for the current tower only.\n",
    "    # 'losses' is the key for collection\n",
    "    # scope is for e.g. 'tower_0'\n",
    "    losses = tf.get_collection('losses', scope)\n",
    "\n",
    "    # Calculate the total loss for the current tower.\n",
    "    total_loss = tf.add_n(losses, name='total_loss')\n",
    "    \n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "        loss_name = re.sub('%s_[0-9]*/' % 'tower_GPU', '', l.op.name)\n",
    "        tf.summary.scalar(loss_name, l)\n",
    "    \n",
    "    #tf.summary.scalar('tower_total_loss', total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Averaging Gradients \n",
    "\n",
    "Calculating the gradients on each tower, results in 4 group of gradients, stored in __tower_grads__. The following  function takes __tower_grads__, and calculates the average gradient for each shared variable across all towers.\n",
    "Notice: that this function provides a synchronization point across all towers.\n",
    "\n",
    " - tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower.\n",
    "\n",
    "This function returns list of pairs of (gradient, variable) where the gradient has been averaged across all towers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Training\n",
    " - The following cell, creates a graph to run one step of training with respect to the loss.\n",
    " - It creates an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    " - When training a model, it is often recommended to lower the learning rate as the training progresses. \n",
    "This function applies an exponential decay function to a provided initial learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    #These varibales are defined on CPU\n",
    "    #with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # create learning_decay\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay( learning_rate,\n",
    "                                     global_step,\n",
    "                                     decay_steps,\n",
    "                                     decay_rate, staircase=True )\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    \n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    " - We choose a unique name for all operations within a tower. e.g. all operations in the first tower are prepended with tower_GPU0. We do it using name_scope(). It defines __operations__ with unique prefix name within each tower on the corresponding GPU. \n",
    " - All variables are placed to the CPU and accessed via tf.get_variable (no variable on GPUs).\n",
    " - we create a **train_op**, where we apply:\n",
    "     - Gradients on all weights and biases in CPU.\n",
    "     - Moving Average on all trainable variables.\n",
    " - **train_op** would be called later in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 40 SETI images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# Get images and labels for CIFAR-10.\n",
    "images, labels = read_inputs()\n",
    "batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue([images, labels], capacity=2 * num_GPUs)\n",
    "\n",
    "# Calculate the gradients for each model tower.\n",
    "with tf.device('/cpu:0'):\n",
    "    towers_grads = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()): # It is defined to share/reuse all the variables on CPU.\n",
    "        for i in xrange(num_GPUs):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('%s_%d' % ('tower_GPU', i)) as nscope:\n",
    "                    # read one batch for the GPU\n",
    "                    #x_batch, y_batch = dataset.train.next_batch(batch_size,shuffle=True)\n",
    "                    #image_batch = tf.reshape(x_batch, [-1,height,width,1], name = 'image_batch') \n",
    "                    image_batch, label_batch = batch_queue.dequeue()\n",
    "                    \n",
    "                    # Build inference part of the Graph.\n",
    "                    logits = inference(image_batch) # it builds a subgraph on twoer with all the operations, to use the shared variables.\n",
    "\n",
    "                    # This function constructs the entire SETI model, calling inference.\n",
    "                    # It shares the variables across all towers (e.g. conv1/weights).\n",
    "                    # Calculate the loss for one tower of the SETI model. \n",
    "                    loss = tower_loss(nscope, logits, label_batch)\n",
    "                    # For the next tower, when we call tower_loss() again, it calls inference(), and tries to create new variables.\n",
    "                    # For example, it tries to create conv1/weights, again.\n",
    "                    # We will reuse variables for the next tower. \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                    # Grads is a list over the gradient calculation for each tower.\n",
    "                    # its shape looks like [(<tensor>,<variable>), (<tensor>,<variable>), ..]\n",
    "                    # where, <tensor> is gradient value of <variable>\n",
    "                    tower_grads = optimizer.compute_gradients(loss)\n",
    "                    towers_grads.append(tower_grads)\n",
    "\n",
    "    # calculate avg gradients on CPU                \n",
    "    avg_grads = average_gradients(towers_grads)\n",
    "    \n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = optimizer.apply_gradients(avg_grads, global_step=global_step)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    with tf.name_scope('Moving_Avg_Decay'):\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(moving_avg_decay, global_step, name= 'variable_averages')\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    # we call train_op in learning process\n",
    "    # tf.group Creates an op that groups multiple operations.\n",
    "    # When this op finishes, all ops in input have finished. This op has no output.\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can check the constructed graph here:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Helper functions for TF Graph visualization\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "# Visualizing the network graph. Be sure expand the \"mixed\" nodes to see their \n",
    "# internal structure. We are going to visualize \"Conv2D\" nodes.\n",
    "\n",
    "graph_def = tf.get_default_graph().as_graph_def()\n",
    "tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x_ph  = tf.placeholder(tf.float32, shape=[None, n_input], name = 'x_ph')\n",
    "y_ph = tf.placeholder(tf.float32, shape=[None, num_classes], name = 'y_ph')\n",
    "def build_evaluation_graph():\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        image_test = tf.reshape(x_ph, [-1,height,width,1], name = 'image_test') \n",
    "        logits_test = inference(image_test)\n",
    "        pred = tf.cast(logits_test, tf.float32)\n",
    "        top_k_op = tf.nn.in_top_k(pred,tf.argmax(y_ph,1) , 1)\n",
    "        return top_k_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    top_k_op = build_evaluation_graph()\n",
    "#x_batch, y_test = dataset.test.next_batch(batch_size)\n",
    "def evaluate():\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        num_iter = int(math.ceil(dataset.test.num_examples / batch_size))\n",
    "        true_count = 0  # Counts the number of correct predictions.\n",
    "        total_sample_count = num_iter * batch_size\n",
    "        step = 0\n",
    "        while step < num_iter:\n",
    "            x_batch, y_batch = dataset.test.next_batch(batch_size)\n",
    "            predictions = sess.run([top_k_op], feed_dict={x_ph: x_batch, y_ph: y_batch})\n",
    "            true_count += np.sum(predictions)\n",
    "            #print true_count\n",
    "            step += 1\n",
    "\n",
    "        # Compute precision @ 1.\n",
    "        precision = true_count*1.0 / total_sample_count\n",
    "        tf.summary.scalar('precision', precision)\n",
    "        return precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "directory = os.path.dirname(chk_directory)\n",
    "try:\n",
    "    os.stat(directory)\n",
    "    ckpt = tf.train.get_checkpoint_state(chk_directory)\n",
    "    print ckpt\n",
    "except:\n",
    "    os.mkdir(directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "loss_values = []\n",
    "\n",
    "X_test = dataset.test.images\n",
    "y_test = dataset.test.labels\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(init)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# Start the queue runners.\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "\n",
    "# load previously trained model if appilcable\n",
    "ckpt = tf.train.get_checkpoint_state(chk_directory)\n",
    "if ckpt:\n",
    "    print \"loading model: \",ckpt.model_checkpoint_path\n",
    "    #saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2000\n",
      "Total epochs: 5000\n",
      "Batch size: 128\n",
      "Total batchs per epoch: 15\n",
      "Epoch: 0001 , g_step: 0015 , Batch (sec)= 0.01682 , lr= 0.005000000 , cost= 22.839546204 , pred= 0.493489583\n",
      "Epoch: 0051 , g_step: 0765 , Batch (sec)= 0.01785 , lr= 0.005000000 , cost= 22.303747177 , pred= 0.404947917\n",
      "Epoch: 0101 , g_step: 1515 , Batch (sec)= 0.01732 , lr= 0.004800000 , cost= 21.228712082 , pred= 0.319010417\n",
      "Epoch: 0151 , g_step: 2265 , Batch (sec)= 0.01702 , lr= 0.004800000 , cost= 20.866416931 , pred= 0.480468750\n",
      "Epoch: 0201 , g_step: 3015 , Batch (sec)= 0.01703 , lr= 0.004608000 , cost= 19.708980560 , pred= 0.687500000\n",
      "Epoch: 0251 , g_step: 3765 , Batch (sec)= 0.01689 , lr= 0.004608000 , cost= 19.473861694 , pred= 0.565104167\n",
      "Epoch: 0301 , g_step: 4515 , Batch (sec)= 0.01749 , lr= 0.004423680 , cost= 18.448333740 , pred= 0.701822917\n",
      "Epoch: 0351 , g_step: 5265 , Batch (sec)= 0.01698 , lr= 0.004423680 , cost= 18.502273560 , pred= 0.678385417\n",
      "Epoch: 0401 , g_step: 6015 , Batch (sec)= 0.01731 , lr= 0.004246733 , cost= 17.885156631 , pred= 0.625000000\n",
      "Epoch: 0451 , g_step: 6765 , Batch (sec)= 0.01681 , lr= 0.004246733 , cost= 17.435285568 , pred= 0.687500000\n",
      "Epoch: 0501 , g_step: 7515 , Batch (sec)= 0.01731 , lr= 0.004076863 , cost= 16.899742126 , pred= 0.625000000\n",
      "Epoch: 0551 , g_step: 8265 , Batch (sec)= 0.01714 , lr= 0.004076863 , cost= 16.138410568 , pred= 0.673177083\n",
      "Epoch: 0601 , g_step: 9015 , Batch (sec)= 0.01669 , lr= 0.003913789 , cost= 16.306840897 , pred= 0.695312500\n",
      "Epoch: 0651 , g_step: 9765 , Batch (sec)= 0.01739 , lr= 0.003913789 , cost= 15.734911919 , pred= 0.664062500\n",
      "Epoch: 0701 , g_step: 10515 , Batch (sec)= 0.01696 , lr= 0.003757237 , cost= 15.349639893 , pred= 0.671875000\n",
      "Epoch: 0751 , g_step: 11265 , Batch (sec)= 0.01754 , lr= 0.003757237 , cost= 15.153637886 , pred= 0.669270833\n",
      "Epoch: 0801 , g_step: 12015 , Batch (sec)= 0.01682 , lr= 0.003606947 , cost= 14.440243721 , pred= 0.677083333\n",
      "Epoch: 0851 , g_step: 12765 , Batch (sec)= 0.01762 , lr= 0.003606947 , cost= 14.590004921 , pred= 0.712239583\n",
      "Epoch: 0901 , g_step: 13515 , Batch (sec)= 0.01680 , lr= 0.003462669 , cost= 14.000185013 , pred= 0.679687500\n",
      "Epoch: 0951 , g_step: 14265 , Batch (sec)= 0.01751 , lr= 0.003462669 , cost= 13.740707397 , pred= 0.695312500\n",
      "Epoch: 1001 , g_step: 15015 , Batch (sec)= 0.01721 , lr= 0.003324162 , cost= 13.735264778 , pred= 0.695312500\n",
      "Epoch: 1051 , g_step: 15765 , Batch (sec)= 0.01706 , lr= 0.003324162 , cost= 12.970208168 , pred= 0.718750000\n",
      "Epoch: 1101 , g_step: 16515 , Batch (sec)= 0.01714 , lr= 0.003191196 , cost= 12.907629967 , pred= 0.703125000\n",
      "Epoch: 1151 , g_step: 17265 , Batch (sec)= 0.01748 , lr= 0.003191196 , cost= 12.807189941 , pred= 0.703125000\n",
      "Epoch: 1201 , g_step: 18015 , Batch (sec)= 0.01738 , lr= 0.003063548 , cost= 12.607128143 , pred= 0.704427083\n",
      "Epoch: 1251 , g_step: 18765 , Batch (sec)= 0.01696 , lr= 0.003063548 , cost= 12.285980225 , pred= 0.704427083\n",
      "Epoch: 1301 , g_step: 19515 , Batch (sec)= 0.01741 , lr= 0.002941006 , cost= 11.909320831 , pred= 0.713541667\n",
      "Epoch: 1351 , g_step: 20265 , Batch (sec)= 0.01712 , lr= 0.002941006 , cost= 11.997787476 , pred= 0.718750000\n",
      "Epoch: 1401 , g_step: 21015 , Batch (sec)= 0.01726 , lr= 0.002823366 , cost= 11.637617111 , pred= 0.722656250\n",
      "Epoch: 1451 , g_step: 21765 , Batch (sec)= 0.01673 , lr= 0.002823366 , cost= 11.515789986 , pred= 0.731770833\n",
      "Epoch: 1501 , g_step: 22515 , Batch (sec)= 0.01728 , lr= 0.002710431 , cost= 11.051591873 , pred= 0.722656250\n",
      "Epoch: 1551 , g_step: 23265 , Batch (sec)= 0.01731 , lr= 0.002710431 , cost= 10.960095406 , pred= 0.742187500\n",
      "Epoch: 1601 , g_step: 24015 , Batch (sec)= 0.01755 , lr= 0.002602014 , cost= 10.904106140 , pred= 0.736979167\n",
      "Epoch: 1651 , g_step: 24765 , Batch (sec)= 0.01740 , lr= 0.002602014 , cost= 10.508481979 , pred= 0.747395833\n",
      "Epoch: 1701 , g_step: 25515 , Batch (sec)= 0.01752 , lr= 0.002497933 , cost= 10.659709930 , pred= 0.729166667\n",
      "Epoch: 1751 , g_step: 26265 , Batch (sec)= 0.01733 , lr= 0.002497933 , cost= 10.315513611 , pred= 0.730468750\n",
      "Epoch: 1801 , g_step: 27015 , Batch (sec)= 0.01705 , lr= 0.002398016 , cost= 10.023420334 , pred= 0.735677083\n",
      "Epoch: 1851 , g_step: 27765 , Batch (sec)= 0.01744 , lr= 0.002398016 , cost= 10.199835777 , pred= 0.743489583\n",
      "Epoch: 1901 , g_step: 28515 , Batch (sec)= 0.01723 , lr= 0.002302095 , cost= 10.069322586 , pred= 0.743489583\n",
      "Epoch: 1951 , g_step: 29265 , Batch (sec)= 0.01779 , lr= 0.002302095 , cost= 9.647163391 , pred= 0.720052083\n",
      "Epoch: 2001 , g_step: 30015 , Batch (sec)= 0.01721 , lr= 0.002210011 , cost= 9.582461357 , pred= 0.738281250\n",
      "Epoch: 2051 , g_step: 30765 , Batch (sec)= 0.01756 , lr= 0.002210011 , cost= 9.373294830 , pred= 0.727864583\n",
      "Epoch: 2101 , g_step: 31515 , Batch (sec)= 0.01697 , lr= 0.002121611 , cost= 9.533966064 , pred= 0.744791667\n",
      "Epoch: 2151 , g_step: 32265 , Batch (sec)= 0.01759 , lr= 0.002121611 , cost= 9.350843430 , pred= 0.731770833\n",
      "Epoch: 2201 , g_step: 33015 , Batch (sec)= 0.01673 , lr= 0.002036746 , cost= 9.376725197 , pred= 0.738281250\n",
      "Epoch: 2251 , g_step: 33765 , Batch (sec)= 0.01669 , lr= 0.002036746 , cost= 9.217885017 , pred= 0.734375000\n",
      "Epoch: 2301 , g_step: 34515 , Batch (sec)= 0.01714 , lr= 0.001955276 , cost= 8.838945389 , pred= 0.746093750\n",
      "Epoch: 2351 , g_step: 35265 , Batch (sec)= 0.01698 , lr= 0.001955276 , cost= 9.055277824 , pred= 0.739583333\n",
      "Epoch: 2401 , g_step: 36015 , Batch (sec)= 0.01764 , lr= 0.001877065 , cost= 8.886543274 , pred= 0.744791667\n",
      "Epoch: 2451 , g_step: 36765 , Batch (sec)= 0.01798 , lr= 0.001877065 , cost= 8.521899223 , pred= 0.729166667\n",
      "Epoch: 2501 , g_step: 37515 , Batch (sec)= 0.01697 , lr= 0.001801983 , cost= 8.781726837 , pred= 0.740885417\n",
      "Epoch: 2551 , g_step: 38265 , Batch (sec)= 0.01757 , lr= 0.001801983 , cost= 8.356276512 , pred= 0.742187500\n",
      "Epoch: 2601 , g_step: 39015 , Batch (sec)= 0.01683 , lr= 0.001729903 , cost= 8.606597900 , pred= 0.738281250\n",
      "Epoch: 2651 , g_step: 39765 , Batch (sec)= 0.01682 , lr= 0.001729903 , cost= 8.400902748 , pred= 0.750000000\n",
      "Epoch: 2701 , g_step: 40515 , Batch (sec)= 0.01727 , lr= 0.001660707 , cost= 8.313881874 , pred= 0.734375000\n",
      "Epoch: 2751 , g_step: 41265 , Batch (sec)= 0.01727 , lr= 0.001660707 , cost= 8.295202255 , pred= 0.736979167\n",
      "Epoch: 2801 , g_step: 42015 , Batch (sec)= 0.01741 , lr= 0.001594279 , cost= 8.137622833 , pred= 0.722656250\n",
      "Epoch: 2851 , g_step: 42765 , Batch (sec)= 0.01717 , lr= 0.001594279 , cost= 8.226068497 , pred= 0.736979167\n",
      "Epoch: 2901 , g_step: 43515 , Batch (sec)= 0.01677 , lr= 0.001530507 , cost= 8.013354301 , pred= 0.761718750\n",
      "Epoch: 2951 , g_step: 44265 , Batch (sec)= 0.01692 , lr= 0.001530507 , cost= 7.967264652 , pred= 0.713541667\n",
      "Epoch: 3001 , g_step: 45015 , Batch (sec)= 0.01716 , lr= 0.001469287 , cost= 7.673408031 , pred= 0.729166667\n",
      "Epoch: 3051 , g_step: 45765 , Batch (sec)= 0.01772 , lr= 0.001469287 , cost= 7.541928768 , pred= 0.752604167\n",
      "Epoch: 3101 , g_step: 46515 , Batch (sec)= 0.01713 , lr= 0.001410516 , cost= 7.839115620 , pred= 0.751302083\n",
      "Epoch: 3151 , g_step: 47265 , Batch (sec)= 0.01720 , lr= 0.001410516 , cost= 7.605195522 , pred= 0.733072917\n",
      "Epoch: 3201 , g_step: 48015 , Batch (sec)= 0.01791 , lr= 0.001354095 , cost= 7.468016148 , pred= 0.742187500\n",
      "Epoch: 3251 , g_step: 48765 , Batch (sec)= 0.01728 , lr= 0.001354095 , cost= 7.676144600 , pred= 0.709635417\n",
      "Epoch: 3301 , g_step: 49515 , Batch (sec)= 0.01741 , lr= 0.001299931 , cost= 7.344890118 , pred= 0.759114583\n",
      "Epoch: 3351 , g_step: 50265 , Batch (sec)= 0.01703 , lr= 0.001299931 , cost= 7.452887058 , pred= 0.727864583\n",
      "Epoch: 3401 , g_step: 51015 , Batch (sec)= 0.01728 , lr= 0.001247934 , cost= 7.292663574 , pred= 0.730468750\n",
      "Epoch: 3451 , g_step: 51765 , Batch (sec)= 0.01758 , lr= 0.001247934 , cost= 7.278039932 , pred= 0.735677083\n",
      "Epoch: 3501 , g_step: 52515 , Batch (sec)= 0.01724 , lr= 0.001198017 , cost= 7.301952839 , pred= 0.733072917\n",
      "Epoch: 3551 , g_step: 53265 , Batch (sec)= 0.01698 , lr= 0.001198017 , cost= 6.961185932 , pred= 0.735677083\n",
      "Epoch: 3601 , g_step: 54015 , Batch (sec)= 0.01706 , lr= 0.001150096 , cost= 7.144044876 , pred= 0.738281250\n",
      "Epoch: 3651 , g_step: 54765 , Batch (sec)= 0.01706 , lr= 0.001150096 , cost= 6.888062954 , pred= 0.738281250\n",
      "Epoch: 3701 , g_step: 55515 , Batch (sec)= 0.01751 , lr= 0.001104092 , cost= 6.964052200 , pred= 0.734375000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3751 , g_step: 56265 , Batch (sec)= 0.01766 , lr= 0.001104092 , cost= 7.175558090 , pred= 0.735677083\n",
      "Epoch: 3801 , g_step: 57015 , Batch (sec)= 0.01776 , lr= 0.001059928 , cost= 6.823955536 , pred= 0.722656250\n",
      "Epoch: 3851 , g_step: 57765 , Batch (sec)= 0.01707 , lr= 0.001059928 , cost= 6.989818096 , pred= 0.740885417\n",
      "Epoch: 3901 , g_step: 58515 , Batch (sec)= 0.01763 , lr= 0.001017531 , cost= 6.917915344 , pred= 0.726562500\n",
      "Epoch: 3951 , g_step: 59265 , Batch (sec)= 0.01730 , lr= 0.001017531 , cost= 6.737689972 , pred= 0.730468750\n",
      "Epoch: 4001 , g_step: 60015 , Batch (sec)= 0.01712 , lr= 0.000976830 , cost= 6.979394436 , pred= 0.739583333\n",
      "Epoch: 4051 , g_step: 60765 , Batch (sec)= 0.01694 , lr= 0.000976830 , cost= 6.621466637 , pred= 0.726562500\n",
      "Epoch: 4101 , g_step: 61515 , Batch (sec)= 0.01690 , lr= 0.000937757 , cost= 6.895664215 , pred= 0.733072917\n",
      "Epoch: 4151 , g_step: 62265 , Batch (sec)= 0.01727 , lr= 0.000937757 , cost= 6.673342228 , pred= 0.753906250\n",
      "Epoch: 4201 , g_step: 63015 , Batch (sec)= 0.01722 , lr= 0.000900246 , cost= 6.699405670 , pred= 0.723958333\n",
      "Epoch: 4251 , g_step: 63765 , Batch (sec)= 0.01753 , lr= 0.000900246 , cost= 6.762005329 , pred= 0.746093750\n",
      "Epoch: 4301 , g_step: 64515 , Batch (sec)= 0.01724 , lr= 0.000864236 , cost= 6.420301914 , pred= 0.716145833\n",
      "Epoch: 4351 , g_step: 65265 , Batch (sec)= 0.01694 , lr= 0.000864236 , cost= 6.671444893 , pred= 0.747395833\n",
      "Epoch: 4401 , g_step: 66015 , Batch (sec)= 0.01701 , lr= 0.000829667 , cost= 6.322110176 , pred= 0.727864583\n",
      "Epoch: 4451 , g_step: 66765 , Batch (sec)= 0.01748 , lr= 0.000829667 , cost= 6.327317238 , pred= 0.733072917\n",
      "Epoch: 4501 , g_step: 67515 , Batch (sec)= 0.01742 , lr= 0.000796480 , cost= 6.415302277 , pred= 0.733072917\n",
      "Epoch: 4551 , g_step: 68265 , Batch (sec)= 0.01664 , lr= 0.000796480 , cost= 6.230042458 , pred= 0.710937500\n",
      "Epoch: 4601 , g_step: 69015 , Batch (sec)= 0.01688 , lr= 0.000764621 , cost= 6.565489769 , pred= 0.742187500\n",
      "Epoch: 4651 , g_step: 69765 , Batch (sec)= 0.01749 , lr= 0.000764621 , cost= 6.446410179 , pred= 0.735677083\n",
      "Epoch: 4701 , g_step: 70515 , Batch (sec)= 0.01696 , lr= 0.000734036 , cost= 6.178986549 , pred= 0.718750000\n",
      "Epoch: 4751 , g_step: 71265 , Batch (sec)= 0.01734 , lr= 0.000734036 , cost= 6.547487259 , pred= 0.739583333\n",
      "Epoch: 4801 , g_step: 72015 , Batch (sec)= 0.01720 , lr= 0.000704675 , cost= 6.149214745 , pred= 0.731770833\n",
      "Epoch: 4851 , g_step: 72765 , Batch (sec)= 0.01712 , lr= 0.000704675 , cost= 6.220583439 , pred= 0.739583333\n",
      "Epoch: 4901 , g_step: 73515 , Batch (sec)= 0.01692 , lr= 0.000676488 , cost= 6.035605431 , pred= 0.734375000\n",
      "Epoch: 4951 , g_step: 74265 , Batch (sec)= 0.01696 , lr= 0.000676488 , cost= 6.153367519 , pred= 0.723958333\n",
      "('Wall Time:', '5553.3', 'sec')\n",
      "Optimization Finished!\n",
      "model saved to tmp/SETI1_train//save/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "print 'Training size:',num_examples\n",
    "print 'Total epochs:', training_epochs\n",
    "print 'Batch size:', batch_size\n",
    "total_batch = int(num_examples / batch_size)\n",
    "print 'Total batchs per epoch:',total_batch\n",
    "# Training cycle\n",
    "tr_start = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_accuracy = 0.\n",
    "\n",
    "    # Loop over all batches in one epoch\n",
    "    \n",
    "    for step in range(total_batch):\n",
    "        t_start = time.time()\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        duration = time.time() - t_start\n",
    "        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "    \n",
    "    \n",
    "    # Save model every x epochs\n",
    "    if epoch >= 0 and epoch % 100 == 0:\n",
    "        saver.save(sess, checkpoint_path, global_step = epoch)\n",
    "    \n",
    "    # Summarize model every x epochs\n",
    "    if epoch >= 0 and epoch % 1 == 0:\n",
    "        summary= sess.run(merged)    \n",
    "        train_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    \n",
    "    # Display model every 1 epochs\n",
    "    if epoch >= 0 and epoch % 50 == 0:\n",
    "        plr = sess.run(lr)\n",
    "        g_step = sess.run(global_step)\n",
    "        loss_values.append(loss_value)\n",
    "        sec_per_batch = duration / num_GPUs\n",
    "        test_eval = evaluate()\n",
    "        print(\"Epoch:\"+ '%04d' % (epoch) + \\\n",
    "        \", Ep_time=\" + \"{:.2f}\".format(end - start) + \\\n",
    "        \", g_step:\" + '%04d' % (g_step) + \\\n",
    "        \", lr=\" + \"{:.9f}\".format(plr) + \\\n",
    "        \", cost=\" + \"{:.3f}\".format(loss_value) + \\\n",
    "        #\", Train_Acc=\" + \"{:.2f}\".format(avg_train_acc) + \\\n",
    "        \", Test_Acc=\" + \"{:.2f}\".format(test_eval) + \n",
    "        \", Batch (sec)=\" + \"{:.3f}\".format(sec_per_batch)  ) \n",
    "        \n",
    "print(\"Wall Time:\",\"{:.1f}\".format(time.time() - tr_start), \"sec\")\n",
    "print(\"Optimization Finished!\")\n",
    "print (\"model saved to {}\".format(checkpoint_path))\n",
    "saver.save(sess, checkpoint_path, global_step = (epoch+1)*step)\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1000543f5b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([np.mean(loss_values[i:i+5]) for i in range(len(loss_values))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Accuracy is depend on the number of epoch that you set in partametrs part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[185   1   5   0]\n",
      " [  0  69 124   0]\n",
      " [  0  80 109   0]\n",
      " [  0   0   0 195]]\n",
      "Precision:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98       191\n",
      "          1       0.46      0.36      0.40       193\n",
      "          2       0.46      0.58      0.51       189\n",
      "          3       1.00      1.00      1.00       195\n",
      "\n",
      "avg / total       0.73      0.73      0.73       768\n",
      "\n",
      "Classification accuracy: 0.726562\n",
      "Log Loss: 0.557203\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    num_iter = int(math.ceil(dataset.test.num_examples / batch_size))\n",
    "    step = 0\n",
    "    pred_dict=[]\n",
    "    while step < num_iter:\n",
    "        #print step\n",
    "        x_batch, y_test = dataset.test.next_batch(batch_size)\n",
    "        image_test = tf.reshape(x_batch, [-1,height,width,1], name = 'image_test') \n",
    "        logits_test = inference(image_test)\n",
    "        pred = tf.cast(logits_test, tf.float32)\n",
    "        pred_lbl = sess.run(pred)\n",
    "        pred_dict.append((y_test, pred_lbl))\n",
    "        step += 1\n",
    "y_pred = [] # predicted\n",
    "y_lb = [] # ground truth labels\n",
    "y_pred_lb = [] # predicted labels\n",
    "for item in pred_dict:\n",
    "    for hotcode_val in item[0]:\n",
    "        y_lb.append(np.argmax(hotcode_val)) # ground truth\n",
    "    for hotcode_val in item[1]:\n",
    "        y_pred.append(hotcode_val)\n",
    "        y_pred_lb.append(np.argmax(hotcode_val)) # ground truth\n",
    "print \"Confusion Matrix:\"\n",
    "print metrics.confusion_matrix(y_true= y_lb, y_pred= y_pred_lb)\n",
    "print \"Precision:\"\n",
    "print metrics.classification_report(y_true= y_lb, y_pred= y_pred_lb)\n",
    "print(\"Classification accuracy: %0.6f\" % metrics.accuracy_score(y_true= y_lb, y_pred= y_pred_lb) )\n",
    "print(\"Log Loss: %0.6f\" % metrics.log_loss(y_true= y_lb, y_pred= y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark:\n",
    "- SETI_multi_gpu_train.py achieves ~70% accuracy after 3k epochs of data (45K steps).\n",
    "- Speed: With batch_size 128.  \n",
    "\n",
    "\n",
    "\n",
    "|          System              |   Step Time (sec/batch)   |         Accuracy                 |\n",
    "| ---------------------------- | :-----------------------: |--------------------------------: |\n",
    "| 4 P100 GPUs w/NVLink np8g4   |       ~0.017              | ~72% at 74K steps  (1.5 hours)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "[Deep Learning with TensorFlow](http://cocl.us/SETI-NIMBIX-ML0102EN) is a free course in __cognitiveclass.ia__ where you can learn TensorFlow and Deep Learning togetherwe.\n",
    "\n",
    "Also, running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](http://cocl.us/SETI-NIMBIX-PowerAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Authors\n",
    "\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone wp-image-2258 \" src=\"https://ibm.box.com/shared/static/tyd41rlrnmfrrk78jx521eb73fljwvv0.jpg\" alt=\"Saeed Aghabozorgi\" width=\"178\" height=\"178\" /></div>\n",
    "#### Saeed Aghabozorgi\n",
    "\n",
    "[Saeed Aghabozorgi](https://ca.linkedin.com/in/saeedaghabozorgi), PhD is Sr. Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients’ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
